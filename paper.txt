Turning Trash into Treasure: Accelerating Inference of Large Language Models
with Token Recycling
Xianzhen Luo
1
, Yixuan Wang
1
, Qingfu Zhu
1*
, Zhiming Zhang
1
,
Xuanyu Zhang
2
,Qing Yang
2
, Dongliang Xu
2
, Wanxiang Che
1
1
Harbin Institute of Technology, Harbin, China
2
Du Xiaoman (Beijing) Science Technology Co., Ltd.
{xzluo, wyx, qfzhu, zmzhang, car}@ir.hit.edu.cn
{zhangxuanyu, yangqing, xudongliang}@duxiaoman.com
Abstract
The rapid growth in the parameters of large language mod-
els (LLMs) has made inference latency a fundamental bottle-
neck, limiting broader application of LLMs. Speculative de-
coding represents a lossless approach to accelerate inference
through a guess-and-verify paradigm, leveraging the paral-
lel capabilities of modern hardware. Some speculative de-
coding methods rely on additional structures to guess draft
tokens, such as small models or parameter-efficient architec-
tures , which need extra training before use. Alternatively,
retrieval-based train-free techniques build libraries from pre-
existing corpora or by n-gram generation. However, they face
challenges like large storage requirements, time-consuming
retrieval, and limited adaptability. Observing that candidate
tokens generated during the decoding process are likely to
reoccur in future sequences, we propose Token Recycling.
This approach stores candidate tokens in an adjacency ma-
trix and employs a breadth-first search (BFS)-like algorithm
on the matrix to construct a draft tree. The tree is then vali-
dated through tree attention. New candidate tokens from the
decoding process are then used to update the matrix. Token
Recycling requires <2MB of additional storage and achieves
approximately 2x speedup across all sizes of LLMs. It signif-
icantly outperforms existing train-free methods by 30% and
even a training method by 25%. It can be directly applied to
any existing LLMs and tasks without the need for adaptation.
1 Introduction
Large Language Models (LLMs) (Brown et al. 2020; Gem-
ini Team et al. 2023; Touvron et al. 2023; Meta 2024) have
made significant contributions to the development of artifi-
cial intelligence, becoming the foundation of numerous ap-
plications such as chatbots, code assistants, agents (Ope-
nAI 2023; Chen et al. 2021; Wang et al. 2024a). How-
ever, due to the auto-regressive decoding strategy, LLMs
can only generate a single token at each decoding step, lead-
ing to slow response times for queries. The latency mainly
comes from transferring billions of parameters from High-
Bandwidth Memory to the accelerator’s cache at each de-
coding step, rather than arithmetic computations (Kim et al.
2024; Shazeer 2019; Cai et al. 2024).
Many approaches (Xu et al. 2024; Frantar and Alistarh
2023; Dao 2024; DeepSeek-AI 2024) seek to reduce the
*
Corresponding author
Neglected
Information
[range values[k
for i rangein (
i in (zip xs
LLM
Draft Model/Retrieval Library
for
Typical Speculative Decoding
Token Recycling (Ours)
for i rangein (
i in (zip keys
LLM
Discard
Retrieve
Update
draft/candidate token
accepted token
rejected token
Top-k Candidate Tokens
Adjacency Matrix
for i k
... ... ...
( keys values
Update
for ...k ( keys
for i j
... ... ...
( a b
Future Generations
Figure 1: A comparison of typical speculative decoding and
Token Recycling. Typical speculative decoding drafts some
tokens and verifies them in parallel in one decoding step.
Unlike speculative decoding which discards candidate to-
kens, Token Recycling stores them in an adjacency matrix.
In future generations, draft tokens are retrieved from the ma-
trix which is updated with new candidate tokens. Token Re-
cycling effectively recycles tokens in the decoding process.
latency, with speculative decoding as a key lossless tech-
nique. This approach employ a guess and verify process to
obtain multiple tokens during a single decoding step (Chen
et al. 2023; Leviathan, Kalman, and Matias 2023; Miao et al.
2024; Xia et al. 2023). It first speculates several subsequent
draft tokens and then verifies them using the original LLMs.
The time cost of verification on multiple tokens is compa-
rable to that of generating one token due to the high par-
allel performance of accelerators. Once some draft tokens
are correct, the whole decoding steps can be greatly reduced
arXiv:2408.08696v1 [cs.CL] 16 Aug 2024

while maintaining the quality. To fully utilize the parallel
capability of accelerators, tree attention slightly adjust the
attention mask to verify multiple draft tokens continuations
in one model forward (Cai et al. 2024; Miao et al. 2024).
Speculative decoding aims not only to maintain qual-
ity but also to minimize the cost of speculation. Some
researchers construct additional model structures to guess
the draft tokens, including small draft models (Leviathan,
Kalman, and Matias 2023; Chen et al. 2023) and parameter-
efficient structures (Cai et al. 2024; Lin et al. 2024). How-
ever, these approaches require resources for additional train-
ing on each LLM. The typical approach to achieve train-
free speculative decoding is retrieve-based. In this case, a
retrieval library is pre-defined to obtain tokens following the
suffix of current content as draft tokens. Several methods
have been proposed in this category, each with its trade-offs:
(i) REST (He et al. 2023) transforms existing corpora into
a retrieval library, but the storage is large, retrieval is time-
consuming, and the library lacks flexibility as it’s static to
any queries. (ii) PLD (Saxena 2023) only retrieves the pre-
vious content with minimal cost. However, it can not pre-
dict new tokens or new combinations of tokens. (iii) Look-
head (Fu et al. 2024) construct and update an n-gram library
through LLMs decoding n times. However, LLMs have to
generate n-grams while in inference, causing low efficiency.
Furthermore, all these speculative decoding approaches
fail to fully utilize candidate tokens, which are multiple
possible next tokens generated by LLMs at each decoding
step. Typically, only the top-1 candidate token from accepted
tokens is selected as the output, while other candidate to-
kens, including all candidate tokens from rejected tokens,
are discarded, such as ‘k’ and ‘keys’ in Figure 1. However,
we observe that when current input tokens reappear in
future generations, the following tokens could be candi-
date tokens generated several steps prior. Based on the
observation, we propose Token Recycling, a method which
utilizes candidate tokens as draft tokens. It stores candidate
tokens in an adjacency matrix. Before each decoding step,
a BFS-like approach retrieves a draft tree from the matrix,
which is then verified using tree attention. Once verified, the
newly generated candidate tokens update the matrix. (i) The
matrix provides a flexible retrieval library that is tailored
to each query and offers low retrieval costs due to its small
size (<2MB). (ii) Compared to using the previous content
solely, candidate tokens naturally include more tokens, pro-
viding many possible continuations. (iii) The construction
and update of our library (matrix) utilize the ‘trash’ tokens
without requiring any additional generation.
We conduct comprehensive experiments on general
benchmark SpecBench (Xia et al. 2024), and specialized
dataset on code domain, MBPP (Chen et al. 2021) with
different sizes of Vicuna (Zheng et al. 2023) and Code
Llama (Roziere et al. 2023) (7b, 13b, 33b/34b). The results
show that Token Recycling greatly exceeds previous train-
free approaches, and improves more than 31% on all sizes.
The speed-up ratio even exceeds the widely used training
approach–Medusa, demonstrating its high efficiency.
Our contributions are summarized below:
• We observe that candidate tokens can be reused as draft
tokens in subsequent sequences. Based on this obser-
vation, we propose a train-free speculative decoding
method, Token Recycling, which saves candidate tokens
and retrieves draft tokens based on the adjacency matrix.
• The matrix requires minimal storage space (<2MB) with
a low retrieval cost, and covers many new tokens. Contin-
uously updating provides a dynamic retrieval space, but
doesn’t need additional generation.
• The experimental results show that Token Recycling
achieves approximately 2x speedup on all sizes of LLMs.
It improves more than 31% compared to previous train-
free approaches and achieves a new SOTA, even exceed-
ing a training approach.
2 Background
In this section, we overview the speculative decoding. We
begin with a formal definition of auto-regressive decod-
ing. Next, we explore speculative decoding, highlighting the
guess-and-verify strategy. Finally, we introduce tree atten-
tion, which verifies multiple draft sequences simultaneously.
2.1 Auto-Regressive Decoding
Auto-Regressive (AR) is the default decoding strategy of
LLMs. At each decoding step t, LLMs calculate the proba-
bility distribution of the next token given the current content
s = (x
0
, x
1
, · · · , x
t
) which x
i 
∈ V:
p
t+1 
= P (x|s; θ).
Here V represents the vocabulary and θ represents the pa-
rameters of the LLM. The next token is selected from p
t+1
depending on the sampling methods. Followed Kou et al.
(2024), we focus on greedy decoding in this paper. The next
token is as follows:
x
t+1 
= argmax p
t+1
.
Candidate tokens refer to the top-k tokens with the highest
probabilities
(x
0
t+1
, x
1
t+1
, . . . , x
k−1
t+1 
) = argtopk(p
t+1
)
where k is the number of candidate tokens. Here, argtopk(·)
is a custom operator that returns the indices of the top-k
highest values in the probability distribution p
t+1
.
2.2 Speculative Decoding
Through the guess and verify approach, speculative de-
coding effectively utilizes the parallel capability of GPUs
to accelerate inference. Given s, it first guesses n sub-
sequent draft tokens (˜x
t+1
, · · · , ˜x
t+n
). The combination
(s, ˜x
t+1
, · · · , ˜x
t+n
) is then sent to LLMs for one forward
pass, resulting in:
p
t+1 
= P (x | s; θ),
˜p
t+i 
= P (x | s, ˜x
t+1
, . . . , ˜x
t+i−1
; θ), for i = 2, . . . , n.
p
t+1 
is the same as AR decoding so the ground-truth x
t+1
can be selected using ‘argmax’. If the draft token ˜x
t+1
matches x
t+1
, then ˜p
t+2 
is assumed to identical to p
t+2 
in
AR decoding. Thus, the next token is selected by: x
t+2 
=

4. Update with candidate tokens
event is at an an
Prefix: ... volunteering as a guest
guest
guest
speaker
at
speaker speak Spe
speaker at for is
speak ings in ers
speak
at ~ ~ ~
for ~ ~ ~
for
... ... ... ...
Current Token Candidate Tokens
Current Content
1. Retrieve based on 'guest'
Tree Attention Mask
Prefix guest speaker speak at for
Merged Sequence
Prefix _guest _speaker _at _a
a local nearby guest
2. Model Forward
guest speaker event speaking
speaker at is event
speak ers at ER
at a an the
for a an multicol
... ... ... ...
Current Token Candidate Tokens
a local nearby guest
Top-k Candidate Tokens
speaker at ers a a
LLM
5. Select the longest correct sequence
guest
speaker
speak
at
for
guest
speaker
at
speak
for
ings in
Spe
is ers
Stage3: Verification and Update
Stage1: Adjacency Matrix Initialization Stage2: Draft Tree Retrieval
speaker
speaker at
speaker for
speak
guest
guest
guest
guest
3. Verify all sequence
ings
ings
0. Inherit from previous query
ings
rare
,
ings , rare fair 
speakguest ings
Figure 2: An overview of Token Recycling. The adjacency matrix, initialized by inheriting from the previous query, stores
candidate tokens. Token Recycling first retrieves a draft tree from the matrix based on the last token of the current content. The
tree is then compressed into a merged sequence with a corresponding tree attention mask and sent to the LLM for a forward
pass. After processing, all possible draft sequences are extracted and verified. The longest correct sequence is selected and
added to the content. Finally, the top-k candidate tokens are used to update the matrix for the next iteration.
argmax ˜p
t+2
. This verify process continues until the draft
token does not match the selected token, indicated by:
x
t+j 
= argmax ˜p
t+j
̸
 
= ˜x
t+j 
.
Ultimately, j new tokens are confirmed in one forward pass.
The time cost of one forward pass with (s, ˜x
t+1
, · · · , ˜x
t+n
)
is nearly the same as with s due to the high parallel perfor-
mance of accelerators. Figure 1 shows an example. Consider
the current content ending with the token ‘for’ and the draft
tokens are [‘i’, ‘in’, ‘range’, ‘(’]. After the forward pass, the
output token for ‘for’ is ‘i’, and for ‘i’ is ‘in’, but for ‘in’ is
‘zip’, which fails to match the guessed ‘range’. ‘i’ and ‘in’
are accepted tokens while the others are rejected. Despite
‘zip’ not matching ‘range’, it is still a valid output generated
by the LLM based on the correct content, resulting in three
tokens [‘i’, ‘in’, ‘zip’] being confirmed in one forward pass.
Overall, speculative decoding effectively accelerates infer-
ence while maintaining the same output as AR decoding.
2.3 Tree Attention
Traditional causal attention masks are designed for linear se-
quences, where each token attends to all previous tokens.
This design restricts speculative decoding to verifying one
sequence at a time. However, as the sequence lengthens dur-
ing draft token generation, the number of potential continu-
ations increases. For example, in the draft tree in Figure 2,
the token following ‘guest’ could be ‘speaker’ or ‘speak’,
while both ‘at’ and ‘for’ could follow ‘speaker’. This creates
a need to verify multiple draft sequences simultaneously.
Tree attention modifies the attention mask to address
this. It compresses multiple sequences into a single merged
sequence, such as [‘guest’, ‘speaker’, ‘speak’, ‘at’, ‘for’,
‘ings’], while preserving a tree structure in the tree atten-
tion mask. Each child node attends only to its parent nodes,
preventing sibling tokens from interfering with each other.
After the LLM processes the merged sequence, all possi-
ble sequences such as ‘guest speaker’, ‘guest speaker at’,
‘guest speaker for’, and ‘guest speak’ and so on, along with
their corresponding output tokens are extracted based on the
tree structure and verified in parallel. The longest correct se-
quence is selected as the final output. Tree attention enables
speculative decoding to verify multiple draft sequences in
one decoding step, significantly enhancing efficiency while
maintaining output quality.
In summary, speculative decoding, through guess and ver-
ify and tree attention, improves the inference latency ro-
bustly and efficiently compared to AR decoding.

3 Methodology
Figure 2 provides an overview of Token Recycling. It lever-
ages a hot-start adjacency matrix to store candidate tokens
and employs a BFS-like algorithm to construct a draft tree.
It utilizes tree attention to verify draft sequences and contin-
uously updates the matrix with new candidate tokens gener-
ated during the decoding process.
3.1 Adjacency Matrix Initialization
The adjacency matrix M is a key component in Token Re-
cycling, used to store top-k candidate tokens for each token
in the vocabulary:
M ∈ V
|V|×k
where k is a user-defined hyperparameter. Each element
M[i, j] indicates that the token V
M [i,j] 
is the j-th candidate
token associated with V
i
. The use of matrix format, as op-
posed to other structures like tries, enables efficient parallel
processing of candidate tokens, which is crucial for reducing
retrieval and update times.
Initially, all elements are set to zero, meaning that a to-
ken must appear in draft tokens before it has valid candidate
tokens. This initialization leads to the matrix starting with
limited predictive capability, potentially causing inefficien-
cies during the early stages of inference. To mitigate this
limitation, we implement a hot start strategy. This involves
inheriting the matrix from the previous query, thereby lever-
aging existing knowledge. Even if queries differ in the do-
main, candidate tokens often include common expressions
and patterns that frequently appear across various queries.
Nevertheless, as the first query lacks a preceding query, the
matrix is set to zero. Consequently, hot start ensures that the
matrix has a broader starting point, covering a wide range of
potential continuations.
3.2 Draft Tree Retrieval
The adjacency matrix M stores candidate tokens, which can
be used as draft tokens when their corresponding tokens ap-
pear later. Directly using the matrix could only determine
the immediate next token, such as finding ‘speaker’ follow-
ing ‘guest’ (see Figure 2). Even if ‘speaker’ is correct, it
only slightly improves upon AR decoding, adding just one
additional token. In fact, the matrix also holds possible con-
tinuations for these candidate tokens, suggesting subsequent
tokens like ‘at’ following ‘speaker’. Extending the sequence
step by step allows for longer draft sequences. Furthermore,
by storing top-k candidate tokens, multiple potential contin-
uations can be explored in parallel for each token, such as
‘at’, and ‘for’ following ‘speaker’. This breadth-first search
(BFS)-like process enables the exploration of the sequence
length and diverse continuations while constructing a draft
tree with the adjacency matrix. Inspired by Cai et al. (2024),
the tree is static and imbalanced. The complete tree structure
is shown in Appendix 8.1.
• Static: The number of children for each node remains
constant across all decoding steps. This fixed structure
facilitates pre-processing, such as preparing attention
masks, and enables efficient parallel operations during
layer traversal. By avoiding the need to individually tra-
verse each node, it significantly reduces retrieval time.
• Imbalance: Nodes positioned earlier in each layer have
more children and extend deeper. Since candidate tokens
are ordered by their probabilities in the matrix, this de-
sign prioritizes the most probable continuations, allocat-
ing computational resources to higher confident paths.
The BFS-like approach for retrieving the draft tree be-
gins with the matrix M and the tree structure T ree. The
retrieval process starts from the root, which is the last to-
ken of current content. Take the token ‘guest’ in Figure 2
as an example. As the root forms the first layer, all can-
didate tokens for ‘guest’ are extracted from M, resulting
in [‘speaker’, ‘speak’, ‘Spe’]. According to T ree, the first
layer allows each token to have two children, Therefore,
‘speaker’ and ‘speak’, which have the top-2 probabilities,
are selected and added to the second layer. The process then
proceeds to expand a new layer starting from [‘speaker’]
and [‘speak’]. All their candidate tokens are retrieved in par-
allel, resulting in [‘at’, ‘for’, ‘is’] and [‘ings’, ‘in’, ‘ers’].
T ree specifies that the first node in this layer (‘speaker’)
can have two children, while the subsequent node (‘speak’)
can only have one child. Consequently, the new layer tokens
are [‘at’, ‘for’], and [‘ings’]. This process repeats until the
specified depth is reached. Finally, a merged sequence S is
constructed through traversing the draft tree by layers. The
detailed Algorithm 1 is provided in Appendix 8.1.
This retrieval method achieves the desired length and va-
riety of the draft sequences, resulting in a single merged se-
quence that can be sent to the LLM.
3.3 Verification and Update
Once the merged sequence S is constructed, it is combined
with the pre-processed attention mask into the LLM for a
forward pass. The verification aligns with Section 2.3. All
potential draft sequences are extracted based on T ree, and
the longest correct sequence is selected as the final output.
Following verification, the adjacency matrix M is up-
dated in parallel based on the output distributions ˜p
i+1 
of
each draft token x
i 
∈ S:
M[˜x
i
] = argtopk(˜p
i+1
).
It is important to note that duplicates may be in S since
multiple preceding tokens may have the same candidate to-
ken. Consequently, the output distributions of duplicates are
likely to differ. When performing updates in parallel, CUDA
operations may merge these updates, leading to variations in
the final result. For example, if x
i 
appears twice and has two
different top-2 output tokens, [y
0
, y
1
], [z
0
, z
1
], then M[x
i
]
could be updated to exactly one of the following results:
[y
0
, z
1
], [y
0
, y
1
], [z
0
, z
1
] or [z
0
, y
1
]. We do not resolve this
merging, as adding controls reduces overall performance, as
discussed later in Section 5.2.
The update process directly overwrites the previous can-
didate tokens, and leverages the new ones as draft tokens for
subsequent decoding steps. This allows the retrieval space
to dynamically adapt to the current content, focusing on the
most relevant and probable continuations. It also eliminates

the necessity for extra operations beyond the standard de-
coding to update the retrieval space.
In summary, Token Recycling capitalizes on the “trash”
present in speculative decoding by implementing a cycling
process between candidate and draft tokens. It accelerates
inference without the need for additional model structures
or training, making it highly adaptable and seamlessly inte-
grated with any architecture or model size.
4 Experiment
4.1 Experimental Setup
Align with previous work (Kou et al. 2024), we focus
on common computational redundancy scenarios, specifi-
cally greedy decoding with a batch size of one. The fol-
lowing evaluation metrics are used: Mean Accepted Token
(MAT) (Xia et al. 2024) represents the average number of
tokens confirmed in a single decoding step; Tokens per Sec-
ond (Ts/s) measures the number of tokens processed per
second; Speedup ratio compares the performance relative
to HuggingFace’s implementation of AR decoding. We set
k = 8 for M (<2MB storage in sum) and the draft tree
structure is shown in Appendix 8.1. All experiments are con-
ducted using Pytorch 2.3 with a single A100-80GB GPU and
128 CPUs under CUDA 12.2.
Datasets and LLMs We conduct experiments on
SpecBench (Xia et al. 2024) and MBPP (Austin et al. 2021).
SpecBench is a comprehensive benchmark encompassing
diverse scenarios including Multi-turn Conversation, Trans-
lation, Summarization, Question Answering, Mathematical
Reasoning, and Retrieval-Augmented Generation. MBPP
is a widely used dataset in code generation, a domain with
a growing demand for efficient generation. These datasets
enable a comparative analysis with prior work across both
general and specialized domains. For the LLMs, we follow
the standard practice of utilizing Vicuna (Chiang et al.
2023) for SpecBench and Code Llama (Roziere et al. 2023)
for MBPP across three different scales: 7B, 13B, and 33B
1
.
Baseline We compare Token Recycling with three loss-
less, train-free, and retrieval-based methods. Lookahead
constructs an n-gram retrieval library through additional
n-gram generation during decoding, consuming significant
computational resources. PLD treats previous content as the
retrieval library, which is constrained and cannot introduce
new tokens or new token combinations. REST builds the re-
trieval library from existing training datasets, requiring large
storage and considerable retrieval time. The static nature
of the library also prevents it from adapting to individual
queries. Furthermore, we include a parameter-efficient tun-
ing baseline for border comparison. Medusa adds multiple
additional LM heads in the final layer to predict draft tokens.
We focus on losses Medusa-1 since Medusa-2 is lossy. All
baselines use their default hyperparameters.
4.2 Main Results
Table 1 shows the performance of Token Recycling com-
pared to other methods. On SpecBench, it achieves more
than a 2x speedup on the 7B model, nearly 30% higher
than the previous train-free methods. Even compared to tun-
ing Medusa, it shows an improvement of almost 25%. For
the 13B and 33B models, it consistently provides nearly
2x speedup, maintaining the 30% acceleration advantage.
These results demonstrate that Token Recycling is the most
effective train-free method on SpecBench, offering substan-
tial and consistent speedup across all model sizes.
Notably, Token Recycling achieves the best speedup
across most sub-tasks as well, except it slightly trails PLD on
Summarization. This may be due to this task often involves
many repetitions of previous content. However, the perfor-
mance gap between Token Recycling and PLD narrows as
the model size increases, reaching only a 1% difference with
the 33B model. This is due to larger models tending to gen-
erate new tokens rather than repeat previous content. In other
tasks such as Multi-turn Conversation, Translation, Question
Answering, and Mathematical Reasoning, Token Recycling
shows a significant improvement of about 40%˜70% for the
7B model. This demonstrates the strong generalization of
our method across various scenarios. Although the improve-
ment on Retrieval-Augmented Generation is less than 3%
for the 7B model, it increases with model size, exceeding
10% for the 33B one. This improvement is consistent with
the preference of larger models for new tokens. Compared to
the general domain, all methods achieve greater acceleration
on the code domain due to its higher content redundancy. To-
ken Recycling provides approximately 2.3x speedup across
all model scales, achieving the SOTA performance.
Furthermore, performances on Trans show the advantages
of our method compared to PLD and REST. While PLD
shows negligible speedup (close to 1x) and REST achieves
its lowest speedup across tasks, Token Recycling consis-
tently delivers over 1.75x speedup across all model sizes.
Notably, on the 7B model, PLD results in a slowdown, and
REST achieves just 1.05x, whereas Token Recycling reaches
1.9x. Trans requires generating new tokens continuously, in-
volving minimal repetition of previous content. Addition-
ally, it is highly context-sensitive, making it challenging to
find exact matches from any pre-existing database. These
pose challenges for PLD and REST. In contrast, the adap-
tive and diverse retrieval space of Token Recycling leads
to superior performance. In addition to Speed, Token Re-
cycling achieves the highest MAT across both benchmarks.
This is attributed to its shorter retrieval times and the avoid-
ance of additional generations like Lookahead. This allows
for deeper and wider draft trees, enabling more tokens to be
accepted in a single decoding step.
Token Recycling demonstrates significant improvements
across all scenarios, highlighting its efficiency and broad ap-
plicability. Importantly, our method is train-free and self-
drafting, allowing for an approximate 2x speedup that can be
seamlessly applied as a “free lunch” to any existing LLM.
1
The largest model of Code Llama is 34B, for consistency and
convenience in our comparisons, we refer to it as 33B.

#Para Method 
SpecBench MBPP
MT Trans Sum QA Math RAG MAT Ts/s Speed MAT Ts/s Speed
7B
Vanilla 1.00 1.00 1.00 1.00 1.00 1.00 1.00 54.30 1.00 1.00 56.15 1.00
Lookahead 1.42 1.12 1.21 1.21 1.52 1.13 1.64 69.03 1.27 1.66 79.16 1.41
PLD 
1.53 0.98 2.36 1.10 1.50 1.74 1.75 83.30 1.53 1.39 66.65 1.19
REST 1.37 1.05 1.12 1.42 1.06 1.30 1.84 66.29 1.22 2.08 87.08 1.55
Medusa
* 
1.90 1.57 1.48 1.58 1.87 1.45 2.31 89.41 1.65 - - -
Ours 2.17 1.90 1.94 1.95 2.40 1.78 2.70 110.06 2.03 2.93 131.20 2.34
shuffle 2.16 1.91 1.93 1.94 2.37 1.72 2.71 109.07 2.01 - - -
13B
Vanilla 
1.00 1.00 1.00 1.00 1.00 1.00 1.00 39.41 1.00 1.00 41.31 1.00
Lookahead 1.29 1.06 1.16 1.12 1.48 1.09 1.63 47.50 1.21 1.73 56.87 1.38
PLD 1.45 1.01 2.10 1.02 1.55 1.65 1.67 57.01 1.45 1.48 52.20 1.26
REST 1.51 1.14 1.31 1.50 1.17 1.50 1.82 53.34 1.35 2.05 70.13 1.70
Medusa
* 
1.94 1.66 1.57 1.62 1.98 1.53 2.39 67.92 1.72 - - -
Ours 
1.98 1.77 1.89 1.75 2.21 1.73 2.72 74.57 1.89 3.08 93.42 2.26
33B
Vanilla 1.00 1.00 1.00 1.00 1.00 1.00 1.00 18.44 1.00 1.00 19.44 1.00
Lookahead 
1.32 1.09 1.20 1.17 1.55 1.14 1.61 23.03 1.25 1.70 29.22 1.50
PLD 1.43 1.06 1.94 1.08 1.55 1.41 1.55 25.89 1.40 1.41 25.89 1.33
REST 1.63 1.27 1.42 1.61 1.29 1.57 1.81 26.99 1.46 2.10 36.85 1.90
Medusa
* 
1.98 1.75 1.63 1.68 2.09 1.61 2.32 33.11 1.80 - - -
Ours 
1.95 1.75 1.92 1.77 2.24 1.78 2.63 35.16 1.91 3.05 45.43 2.34
Table 1: Performance comparison of different methods on SpecBench (Vicuna) and on MBPP (Code Llama) across three pa-
rameter sizes. Vanilla represents AR decoding. Metrics include Mean Accepted Token (MAT), Tokens per Second (Ts/s), and
Speedup ratio relative to Vanilla (Speed). Categories for SpecBench include cover Multi-turn Conversation (MT), Translation
(Trans), Summarization (Sum), Question Answering (QA), Mathematical Reasoning (Math), and Retrieval-Augmented Gen-
eration (RAG), with Speed as the displayed metric. MBPP results exclude Medusa as it lacks a Code Llama variant. Medusa
*
involves training while others are training-free. shuffle indicates performance after shuffling the data order (Section 5.2).
5 Analysis
5.1 Tree Structure
As previously outlined in Section 3.2, our tree structure is
static and imbalanced. The tree size is a crucial factor to ac-
celerate. A larger tree allows more tokens confirmed in one
decoding step but also introduces more computational over-
head, increasing the time required for each decoding step.
To investigate the impact of tree size, specifically its depth
and breadth on acceleration performance, experiments are
conducted on MT-Bench using Vicuna-7B.
Breadth Increasing the breadth of the tree allows for cov-
ering more possibilities during decoding. In Figure 3(a),
the breadth is expanded by adding nodes while keeping
the depth fixed at six layers. This leads to a consistent im-
provement in MAT. However, when the breadth exceeds 80,
Tokens/s begins to decrease. The additional computational
overhead eventually outweighs the benefits of a higher MAT.
Depth Increasing the depth of the tree allows for accept-
ing longer sequences during decoding. In Figure 3(b), with
the number of nodes fixed at 80, the depth is gradually in-
creased. MAT initially rises rapidly but eventually shows
minimal improvement, while Tokens/s noticeably fluctuates.
Because the matrix stores candidate tokens for only adjacent
steps, longer sequences weaken the connections between
distant tokens. This limitation reduces the effectiveness of
increased depth, causing Tokens/s to fluctuate.
5.2 Ablation Study
Data Order In Token Recycling, the adjacency matrix is
initialized with hot start, meaning it inherits the matrix from
the previous query. By default, SpecBench organizes data
so that instances from the same domain are processed con-
secutively. This sequential ordering might allow domain-
specific patterns to persist, potentially optimizing token re-
trieval and boosting performance. To evaluate the robustness
of hot start, we conduct an experiment where all data in-
stances are shuffled across different domains. This shuffling
ensures that each query is independent, removing any poten-
tial domain influence from the previous query. As shown in
Table 1 under shuffle, the performance impact is minimal,
with the Speed decreasing by less than 1%. This demon-
strates that hot start is effective and robust, even without
domain continuity between queries.
Update Strategies Section 3.1 discuss duplicate tokens
in the merged sequence during matrix updates. We com-
pare three updating strategies: using candidate tokens from
the first occurrence, from the last occurrence, and the cur-
rent method (merging via parallel CUDA operations). Fig-
ure 3(c) indicates that using the last occurrence yields the
highest MAT, which may benefit from more contextual in-
formation. However, the differences among different strate-
gies in MAT are minimal. In terms of Tokens/s, the cur-
rent approach significantly outperforms the other two, as it
avoids the additional processing required to manage token
positions, thereby reducing delays. Speculative decoding is

30 40 50 60 70 80 90
Breadth
2.20
2.40
2.60
2.80
MAT
(a)
4 5 6 7 8
Depth
2.67
2.69
2.71
2.73
(b)
First Cur Last
Strategy
2.68
2.71
2.74
2.77
(c)
102
108
114
120
115
117
119
121
111
114
117
120
Tokens/s
MAT
Tokens/s
Figure 3: Effects of tree breadth and depth on MAT and Tokens/s in (a) and (b), and the impact of updating strategies in (c).
Preprocess (1.6%)
Retrieval (1.6%)
Update (1.5%)
Verify (5.2%)
Model Forward (90.1%)
Figure 4: Time allocation for each operation when LLMs
response to a query.
highly sensitive to latency, any extra operation must provide
substantial benefits to outweigh its time cost.
5.3 Time Allocation
For speculative decoding to be effective, it is essential to
maintain a high hit rate while minimizing the time spent
on additional operations. We divide each decoding step into
several components: preprocessing , retrieving draft tokens,
model forward pass, verifying draft sequences, and updating
the matrix, input tokens, and key-value cache. The average
time spent on each component is shown in Figure 4. The re-
sults indicate that the majority of the time is consumed by
the model forward pass. The verification process also takes
a significant amount of time due to the need to extract and
verify all feasible paths. Retrieving draft tokens and updat-
ing operations take roughly the same amount of time.
6 Related Work
Efficient inference is crucial for real-time applications and
low-resource scenarios. Many strategies have been devel-
oped to reduce latency (Zhou et al. 2024b). Among these,
speculative decoding (Chen et al. 2023; Leviathan, Kalman,
and Matias 2023; Miao et al. 2024; Xia et al. 2023) has
emerged as a losses technique for achieving lossless speedup
by predicting multiple possible continuations simultane-
ously, thus reducing the number of decoding steps needed
without compromising accuracy. Some speculative decod-
ing methods rely on additional draft models to guess draft
tokens. These typically involve using smaller models from
the same series (Zhao et al. 2024; Spector and Re; Sun et al.
2023; Liu et al. 2024b; Yuan et al. 2024; Gong et al. 2024)
or training new models with a shared vocabulary (Leviathan,
Kalman, and Matias 2023; Chen et al. 2023; Zhou et al.
2024a; Li et al. 2024). Additionally, Kou et al. (2024); Wang
et al. (2024b) propose training the original LLMs to enable
non-aggressive decoding. While effective, these approaches
require managing or training multiple models, which can be
non-trivial and resource-intensive. Other methods focus on
parameter-efficient structures. These approaches minimize
the need for complete retraining but still require model-
specific training and adaptation, limiting their scalability and
general applicability (Lin et al. 2024; Liu et al. 2024a).
There are also train-free methods that construct retrieval
libraries to obtain draft tokens (Yang et al. 2023). Looka-
head (Fu et al. 2024) uses the LLM to generate n-grams
through multiple decodings, building a retrieval library that
can hit multiple tokens in one step. Despite this advantage,
it requires the LLM to generate n-grams while responding
to queries, which reduces efficiency. PLD (Saxena 2023)
retrieves only from previous content, resulting in minimal
overhead and significant speedup in high-redundancy tasks
like summarization. However, it provides little acceleration
for tasks requiring the generation of new content, like trans-
lation. REST (He et al. 2023) constructs retrieval libraries
using existing corpora and performs well in common sce-
narios. However, this approach requires large storage, time-
consuming retrieval, and cannot adapt to each query.
Token Recycling is a train-free, retrieval-based method
that stores candidate tokens in an adjacency matrix and
reuses them as draft tokens. It requires no additional gener-
ation, covers a broader range of possible continuations, and
demands minimal storage with low retrieval costs. The up-
date process ensures the retrieval space remains adaptable
7 Conclusion
In this work, we introduce Token Recycling, a speculative
decoding method for accelerating the inference of LLMs. It
utilizes an adjacency matrix to store candidate tokens and re-
trieve a draft tree, which is then verified with tree attention.

The matrix is updated with new candidate tokens generated
during decoding. Token Recycling could be integrated seam-
lessly with existing LLMs and various tasks. As a train-free
approach, it achieves a speedup of approximately 2x with
<2MB of additional storage, improving over 31% compared
to previous train-free approaches.
References
Austin, J.; Odena, A.; Nye, M.; Bosma, M.; Michalewski,
H.; Dohan, D.; Jiang, E.; Cai, C.; Terry, M.; Le, Q.; et al.
2021. Program synthesis with large language models. arXiv
preprint arXiv:2108.07732.
Brown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.;
Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,
A.; Agarwal, S.; Herbert-Voss, A.; Krueger, G.; Henighan,
T.; Child, R.; Ramesh, A.; Ziegler, D. M.; Wu, J.; Winter,
C.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.;
Chess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford,
A.; Sutskever, I.; and Amodei, D. 2020. Language Mod-
els are Few-Shot Learners. In Larochelle, H.; Ranzato, M.;
Hadsell, R.; Balcan, M.; and Lin, H., eds., Advances in Neu-
ral Information Processing Systems 33: Annual Conference
on Neural Information Processing Systems 2020, NeurIPS
2020, December 6-12, 2020, virtual.
Cai, T.; Li, Y.; Geng, Z.; Peng, H.; Lee, J. D.; Chen,
D.; and Dao, T. 2024. Medusa: Simple LLM Inference
Acceleration Framework with Multiple Decoding Heads.
arXiv:2401.10774.
Chen, C.; Borgeaud, S.; Irving, G.; Lespiau, J.-B.; Sifre, L.;
and Jumper, J. 2023. Accelerating Large Language Model
Decoding with Speculative Sampling. arXiv:2302.01318.
Chen, M.; Tworek, J.; Jun, H.; Yuan, Q.; Pinto, H. P. d. O.;
Kaplan, J.; Edwards, H.; Burda, Y.; Joseph, N.; Brock-
man, G.; Ray, A.; Puri, R.; Krueger, G.; Petrov, M.; Khlaaf,
H.; Sastry, G.; Mishkin, P.; Chan, B.; Gray, S.; Ryder, N.;
Pavlov, M.; Power, A.; Kaiser, L.; Bavarian, M.; Winter, C.;
Tillet, P.; Such, F. P.; Cummings, D.; Plappert, M.; Chantzis,
F.; Barnes, E.; Herbert-Voss, A.; Guss, W. H.; Nichol, A.;
Paino, A.; Tezak, N.; Tang, J.; Babuschkin, I.; Balaji, S.;
Jain, S.; Saunders, W.; Hesse, C.; Carr, A. N.; Leike, J.;
Achiam, J.; Misra, V.; Morikawa, E.; Radford, A.; Knight,
M.; Brundage, M.; Murati, M.; Mayer, K.; Welinder, P.; Mc-
Grew, B.; Amodei, D.; McCandlish, S.; Sutskever, I.; and
Zaremba, W. 2021. Evaluating Large Language Models
Trained on Code. arXiv:2107.03374.
Chiang, W.-L.; Li, Z.; Lin, Z.; Sheng, Y.; Wu, Z.; Zhang, H.;
Zheng, L.; Zhuang, S.; Zhuang, Y.; Gonzalez, J. E.; Stoica,
I.; and Xing, E. P. 2023. Vicuna: An Open-Source Chatbot
Impressing GPT-4 with 90%* ChatGPT Quality.
Dao, T. 2024. FlashAttention-2: Faster Attention with Better
Parallelism and Work Partitioning. In International Confer-
ence on Learning Representations (ICLR).
DeepSeek-AI. 2024. DeepSeek-V2: A Strong, Econom-
ical, and Efficient Mixture-of-Experts Language Model.
arXiv:2405.04434.
Frantar, E.; and Alistarh, D. 2023. SparseGPT: Mas-
sive Language Models Can be Accurately Pruned in One-
Shot. In Krause, A.; Brunskill, E.; Cho, K.; Engelhardt,
B.; Sabato, S.; and Scarlett, J., eds., Proceedings of the
40th International Conference on Machine Learning, vol-
ume 202 of Proceedings of Machine Learning Research,
10323–10337. PMLR.
Fu, Y.; Bailis, P.; Stoica, I.; and Zhang, H. 2024. Break the
Sequential Dependency of LLM Inference Using Lookahead
Decoding. arXiv:2402.02057.
Gemini Team; Anil, R.; Borgeaud, S.; Wu, Y.; Alayrac, J.-
B.; Yu, J.; Soricut, R.; Schalkwyk, J.; Dai, A. M.; Hauth, A.;
et al. 2023. Gemini: a family of highly capable multimodal
models. arXiv preprint arXiv:2312.11805.
Gong, Z.; Liu, J.; Wang, Z.; Wu, P.; Wang, J.; Cai, X.; Zhao,
D.; and Yan, R. 2024. Graph-Structured Speculative De-
coding. In Findings of the Association for Computational
Linguistics ACL 2024, 11404–11415.
He, Z.; Zhong, Z.; Cai, T.; Lee, J. D.; and He, D. 2023.
Rest: Retrieval-based speculative decoding. arXiv preprint
arXiv:2311.08252.
Kim, S.; Hooper, C.; Gholami, A.; Dong, Z.; Li, X.; Shen,
S.; Mahoney, M. W.; and Keutzer, K. 2024. SqueezeLLM:
Dense-and-Sparse Quantization. arXiv:2306.07629.
Kou, S.; Hu, L.; He, Z.; Deng, Z.; and Zhang, H.
2024. CLLMs: Consistency Large Language Models.
arXiv:2403.00835.
Leviathan, Y.; Kalman, M.; and Matias, Y. 2023. Fast Infer-
ence from Transformers via Speculative Decoding. In Pro-
ceedings of the 40th International Conference on Machine
Learning, 19274–19286. PMLR.
Li, Y.; Wei, F.; Zhang, C.; and Zhang, H. 2024. EAGLE:
Speculative Sampling Requires Rethinking Feature Uncer-
tainty. In Forty-first International Conference on Machine
Learning.
Lin, F.; Yi, H.; Li, H.; Yang, Y.; Yu, X.; Lu, G.; and Xiao, R.
2024. BiTA: Bi-Directional Tuning for Lossless Accelera-
tion in Large Language Models. arXiv:2401.12522.
Liu, F.; Tang, Y.; Liu, Z.; Ni, Y.; Han, K.; and Wang, Y.
2024a. Kangaroo: Lossless Self-Speculative Decoding via
Double Early Exiting. arXiv:2404.18911.
Liu, X.; Hu, L.; Bailis, P.; Cheung, A.; Deng, Z.; Stoica,
I.; and Zhang, H. 2024b. Online Speculative Decoding. In
Salakhutdinov, R.; Kolter, Z.; Heller, K.; Weller, A.; Oliver,
N.; Scarlett, J.; and Berkenkamp, F., eds., Proceedings of the
41st International Conference on Machine Learning, vol-
ume 235 of Proceedings of Machine Learning Research,
31131–31146. PMLR.
Meta. 2024. Introducing Meta Llama 3: The most capable
openly available LLM to date.
Miao, X.; Oliaro, G.; Zhang, Z.; Cheng, X.; Wang, Z.;
Zhang, Z.; Wong, R. Y. Y.; Zhu, A.; Yang, L.; Shi, X.;
Shi, C.; Chen, Z.; Arfeen, D.; Abhyankar, R.; and Jia, Z.
2024. SpecInfer: Accelerating Large Language Model Serv-
ing with Tree-based Speculative Inference and Verification.
In Proceedings of the 29th ACM International Conference
on Architectural Support for Programming Languages and
Operating Systems, Volume 3, volume 3 of ASPLOS ’24,
932–949. Association for Computing Machinery.

OpenAI. 2023. GPT-4 Technical Report. CoRR,
abs/2303.08774.
Roziere, B.; Gehring, J.; Gloeckle, F.; Sootla, S.; Gat, I.; Tan,
X. E.; Adi, Y.; Liu, J.; Remez, T.; Rapin, J.; et al. 2023. Code
llama: Open foundation models for code. arXiv preprint
arXiv:2308.12950.
Saxena, A. 2023. Prompt Lookup Decoding.
Shazeer, N. 2019. Fast Transformer Decoding: One Write-
Head Is All You Need. arXiv:1911.02150.
Spector, B. F.; and Re, C. ???? Accelerating LLM Inference
with Staged Speculative Decoding. In Workshop on Efficient
Systems for Foundation Models@ ICML2023.
Sun, Z.; Suresh, A. T.; Ro, J. H.; Beirami, A.; Jain, H.; and
Yu, F. 2023. SpecTr: Fast Speculative Decoding via Optimal
Transport. In Thirty-seventh Conference on Neural Informa-
tion Processing Systems.
Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.;
Babaei, Y.; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale,
S.; Bikel, D.; Blecher, L.; Canton-Ferrer, C.; Chen, M.; Cu-
curull, G.; Esiobu, D.; Fernandes, J.; Fu, J.; Fu, W.; Fuller,
B.; Gao, C.; Goswami, V.; Goyal, N.; Hartshorn, A.; Hos-
seini, S.; Hou, R.; Inan, H.; Kardas, M.; Kerkez, V.; Khabsa,
M.; Kloumann, I.; Korenev, A.; Koura, P. S.; Lachaux, M.;
Lavril, T.; Lee, J.; Liskovich, D.; Lu, Y.; Mao, Y.; Martinet,
X.; Mihaylov, T.; Mishra, P.; Molybog, I.; Nie, Y.; Poul-
ton, A.; Reizenstein, J.; Rungta, R.; Saladi, K.; Schelten, A.;
Silva, R.; Smith, E. M.; Subramanian, R.; Tan, X. E.; Tang,
B.; Taylor, R.; Williams, A.; Kuan, J. X.; Xu, P.; Yan, Z.;
Zarov, I.; Zhang, Y.; Fan, A.; Kambadur, M.; Narang, S.; Ro-
driguez, A.; Stojnic, R.; Edunov, S.; and Scialom, T. 2023.
Llama 2: Open Foundation and Fine-Tuned Chat Models.
CoRR, abs/2307.09288.
Wang, L.; Ma, C.; Feng, X.; Zhang, Z.; Yang, H.; Zhang,
J.; Chen, Z.; Tang, J.; Chen, X.; Lin, Y.; Zhao, W. X.; Wei,
Z.; and Wen, J. 2024a. A Survey on Large Language Model
Based Autonomous Agents. Frontiers of Computer Science,
18: 186345.
Wang, Y.; Luo, X.; Wei, F.; Liu, Y.; Zhu, Q.; Zhang, X.;
Yang, Q.; Xu, D.; and Che, W. 2024b. Make Some Noise:
Unlocking Language Model Parallel Inference Capability
through Noisy Training. arXiv preprint arXiv:2406.17404.
Xia, H.; Ge, T.; Wang, P.; Chen, S.-Q.; Wei, F.; and Sui, Z.
2023. Speculative Decoding: Exploiting Speculative Exe-
cution for Accelerating Seq2seq Generation. In Bouamor,
H.; Pino, J.; and Bali, K., eds., Findings of the Association
for Computational Linguistics: EMNLP 2023, 3909–3925.
Singapore: Association for Computational Linguistics.
Xia, H.; Yang, Z.; Dong, Q.; Wang, P.; Li, Y.; Ge, T.; Liu, T.;
Li, W.; and Sui, Z. 2024. Unlocking Efficiency in Large Lan-
guage Model Inference: A Comprehensive Survey of Spec-
ulative Decoding. arXiv:2401.07851.
Xu, Y.; Han, X.; Yang, Z.; Wang, S.; Zhu, Q.; Liu, Z.; Liu,
W.; and Che, W. 2024. OneBit: Towards Extremely Low-bit
Large Language Models. arXiv:2402.11295.
Yang, N.; Ge, T.; Wang, L.; Jiao, B.; Jiang, D.; Yang, L.;
Majumder, R.; and Wei, F. 2023. Inference with Refer-
ence: Lossless Acceleration of Large Language Models.
arXiv:2304.04487.
Yuan, H.; Lu, K.; Huang, F.; Yuan, Z.; and Zhou, C. 2024.
Speculative Contrastive Decoding. In Ku, L.-W.; Martins,
A.; and Srikumar, V., eds., Proceedings of the 62nd Annual
Meeting of the Association for Computational Linguistics
(Volume 2: Short Papers). Bangkok, Thailand: Association
for Computational Linguistics.
Zhao, W.; Huang, Y.; Han, X.; Xiao, C.; Liu, Z.; and Sun, M.
2024. Ouroboros: Speculative Decoding with Large Model
Enhanced Drafting. arXiv preprint arXiv:2402.13720.
Zheng, L.; Chiang, W.-L.; Sheng, Y.; Zhuang, S.; Wu,
Z.; Zhuang, Y.; Lin, Z.; Li, Z.; Li, D.; Xing, E. P.;
Zhang, H.; Gonzalez, J. E.; and Stoica, I. 2023. Judg-
ing LLM-as-a-Judge with MT-Bench and Chatbot Arena.
arXiv:2306.05685.
Zhou, Y.; Lyu, K.; Rawat, A. S.; Menon, A. K.; Ros-
tamizadeh, A.; Kumar, S.; Kagy, J.-F.; and Agarwal, R.
2024a. DistillSpec: Improving Speculative Decoding via
Knowledge Distillation. In The Twelfth International Con-
ference on Learning Representations.
Zhou, Z.; Ning, X.; Hong, K.; Fu, T.; Xu, J.; Li, S.; Lou,
Y.; Wang, L.; Yuan, Z.; Li, X.; et al. 2024b. A survey on
efficient inference for large language models. arXiv preprint
arXiv:2404.14294.
8 Appendix
8.1 Draft Tree Algorithm and Structure
Utilizing tree attention (Miao et al. 2024) to extend the path
in the verification phase has become a widely adopted strat-
egy for speculative decoding methods. In Token Recycling,
we also use a heuristically constructed token tree to per-
form the verification. As shown in Figure 5, we construct
a static and unbalanced tree inspired by Cai et al. (2024).
The number k on a node indicates that it is the k-th can-
didate token for its parent node. Based on the phenomenon
that high-confidence tokens have a higher rate of correct-
ness, we assign more child nodes to nodes with high scores
(e.g. node 0) and the reverse for those with low scores (e.g.
node 7). Overall, the tree we construct contains 80 nodes
(including the root node) in 6 layers. This means that each
forward requires an additional draft input of 79 tokens with
a maximum acceptance length of 6.
Building on the tree template described above, we com-
plete the merging of the current inputs by a BFS algorithm
in the inference phase. As described in Algorithm 1, We in-
fill the child nodes of each layer in turn according to the
Adjacency Matrix. The index of the Adjacency Matrix coin-
cides with the top-k of the model’s output token, making it
feasible to construct the sequence S of the sparse token tree
on demand.

Root
0
0
0
0
0
0 1
1 2
1
0
0
2
0
3 4
1
0
0
1
2
0
3
0
4
0
5 6 7
1
0
0
0
1 2
2
0
0
1
3
0
4
0
5
0
6
0
7
0
1
0
0
0
0
1
1
0
2 3
2
0
0
0
1 2
3
0
0
0
1
4
0
0
5
0
0
6
0
7
0
Figure 5: The static tree used in Token Recycling.
Algorithm 1: Static Tree Based BFS
Require: Adjacency matrix M, Static tree structure T ree,
the last prompt token x
t
Ensure: Merged Sequence S
1: Initialize S ← ∅
2: Initialize root ← x
t
3: Initialize the current layer L ← (root)
4: Initialize the current depth d ← 0
5: while d < T ree.depth do
6: Initialize next layer L
next 
← ∅
7: Get all candidate tokens of L from M in parallel
8: xs = M [L]
9: Extract next layer tokens from xs with T ree
10: L
next 
= xs[T ree[d].index]
11: Concatenate S and L
12: S ← (S; L)
13: L ← L
next
14: end while
15: return S